---
jupytext:
  cell_metadata_filter: -all
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.10.3
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---
(reproductibilite-controverses-chapitre)=
# ReproductibilitÃ© et controverses
<table>
  <tr>
    <td align="center">
      <a href="https://github.com/elisabethloranger">
        <img src="https://avatars.githubusercontent.com/u/90270981?v=4?s=100" width="100px;" alt=""/>
        <br /><sub><b>Ã‰lisabeth Loranger</b></sub>
      </a>
      <br />
        <a title="Contenu">ğŸ¤”</a>
    </td>
    <td align="center">
      <a href="https://github.com/pbellec">
        <img src="https://avatars.githubusercontent.com/u/1670887?v=4?s=100" width="100px;" alt=""/>
        <br /><sub><b>Pierre bellec</b></sub>
      </a>
      <br />
        <a title="Contenu">ğŸ¤”</a>
    </td>
  </tr>
</table>

Durant ce cours, on a passÃ© en revue diverses techniques de neuroimagerie qui ouvrent une fenÃªtre fascinante sur la structure et la fonction du cerveau. Mais ces techniques sont rÃ©guliÃ¨rement impliquÃ©es dans des articles scientifiques qui semblent peu crÃ©dibles. Dans ce cours nous allons discuter des controverses autour de la neuroimagerie, et plus gÃ©nÃ©ralement de la crise de reproducibilitÃ© en sciences.

Les objectifs de ce cours sont les suivants :
- Comprendre la crise de reproductibilitÃ© en sciences.
- Comprendre certaines pratiques scientifiques douteuses qui participent
au manque de reproductibilitÃ© en neurosciences cognitives.
- ConnaÃ®tre certains outils qui peuvent amÃ©liorer la reproductibilitÃ© en neurosciences cognitives.

## La crise de reproductibilitÃ©

### Une crise? Quelle crise?
```{figure} ./reproductibilite/significant.png
---
width: 600px
name: significant-fig
---
Cette figure illustre le processus qui amÃ¨ne Ã  un rÃ©sultat scientifique controversÃ© (et le problÃ¨me de comparaisons multiples). Cette figure est tirÃ©e de [xkcd webcomic](https://xkcd.com/882/), sous licence [CC-BY-NC 2.5](https://creativecommons.org/licenses/by-nc/2.5/).
```
En 2016, un sondage auprÃ¨s de 1576 chercheurs a Ã©tÃ© menÃ© dans le but de voir si, dans la
perception des professionnels dans la recherche, il y a une crise de
reproductibilitÃ© et si oui, laquelle ([Baker, 2016](https://www.nature.com/articles/533452a#change-history)). En tout, 90% des chercheurs dans ce sondage pensent
quâ€™il y a effectivement une crise de reproductibilitÃ© (52% pour une crise significative et 38% pour une crise modÃ©rÃ©e).

La reproductibilitÃ©, câ€™est quoi ? Si on avait accÃ¨s
aux donnÃ©es derriÃ¨re ce papier, est-ce quâ€™on serait capable de refaire les
analyses et arriver aux mÃªmes conclusions ? Un autre concept proche est la rÃ©plication: en recrutant de nouveaux sujets et en faisant exactement ce que les autres chercheurs ont fait au niveau des outils utilisÃ©s et des analyses effectuÃ©es, est-ce qu'on va trouver les mÃªmes rÃ©sultats ? Dans le sondage, 70% des personnes sondÃ©es rapportent avoir Ã©chouÃ© Ã  reproduire les rÃ©sultats d'une autre Ã©quipe de recherche, et plus de 50% rapportent avoir Ã©chouÃ© Ã  reproduire leurs propres rÃ©sultats.

Les personnes sondÃ©es ont aussi Ã©valuÃ© les causes probables de cette crise de
reproductibilitÃ©. Parmi les raisons les plus frÃ©quemment mentionnÃ©es,
on retrouve la _pression Ã  publier_ et la _publication sÃ©lective_ (les gens publient
seulement ce qui fonctionne bien) ainsi que la _puissance statistique limitÃ©e_.
Ce chapitre va expliquer certaines de ces notions plus en dÃ©tails, en dÃ©marrant par formaliser le processus de gÃ©nÃ©ration de connaissances scientifiques.

### La mÃ©thode scientifique
```{figure} ./reproductibilite/researchcycle_original.png
---
width: 800px
name: researchcycle-original-fig
---
Cette figure illustre le cycle des dÃ©couvertes scientifiques, selon l'approche de la mÃ©thode scientifique dÃ©crite par [Karl Popper](https://fr.wikipedia.org/wiki/Karl_Popper#Philosophie_des_sciences). Figure adaptÃ©e d'un travail original par [scriberia](https://info.scriberia.com/contact-us) dans le cadre du livre [The Turing way](https://the-turing-way.netlify.app) sous licence [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/), DOI: [10.5281/zenodo.3332807](https://doi.org/10.5281/zenodo.3332807). La figure adaptÃ©e par P. Bellec est elle-mÃªme disponible sous licence [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/).
```
La figure {numref}`researchcycle-original` prÃ©sente une version simplifiÃ©e de la mÃ©thode scientifique pour la dÃ©couverte de connaissances, inspirÃ©e par la thÃ©orie de [Karl Popper](https://fr.wikipedia.org/wiki/Karl_Popper#Philosophie_des_sciences), telle qu'elle est gÃ©nÃ©ralement implÃ©mentÃ©e dans la communautÃ© de recherche.
 * On commence avec les publications, qui reprÃ©sentent les connaissances qui ont Ã©tÃ© accumulÃ©es par dâ€™autres.
 * En lisant cette litÃ©rature, les chercheuses/chercheurs peuvent apprendre ce qui a dÃ©jÃ  Ã©tÃ© dÃ©couvert, et faire des hypothÃ¨ses sur des choses quâ€™on ne connait pas encore.
 * Les chercheuses/chercheurs vont alors formuler un devis de recherche : nombre de participants, groupes, tests statistiques, etc. Elles/ils vont aussi faire des prÃ©dicitions concernant les rÃ©sultats quâ€™elles/ils pensent obtenir.
 * Une fois le devis de recherche Ã©laborÃ©, il est temps de recueillir les donnÃ©es.
 * Ensuite, on analyse les donnÃ©es en suivant le protocole qui avait Ã©tÃ© Ã©tabli dans le devis de recherche.
 * Il faut alors interprÃ©ter les rÃ©sultats, et notamment les comparer Ã  nos prÃ©dictions pour valider ou invalider nos hypothÃ¨ses.
 * Les rÃ©sultats de la recherche sont alors publiÃ©s pour permettre au reste de la communautÃ© de recherche de continuer Ã  formuler de nouvelles hypothÃ¨ses.

 Comme on utilise des statistiques rigoureuses dans cette approche, on ne gÃ©nÃ¨re qu'une quantitÃ© limitÃ© de faux positifs, et donc on fait des dÃ©couvertes scientifiques sans faire trop d'erreurs. En pratique, cette approche peut Ãªtre adaptÃ©e de nombreuses maniÃ¨res avec _des pratiques de recherche douteuses_ qui vont compromettre l'intÃ©gritÃ© et la rigueur des conclusions de l'Ã©tude.

### La mÃ©thode scientifique: hacked
```{figure} ./reproductibilite/researchcycle_hacked.png
---
width: 800px
name: researchcycle-hacked-fig
---
Cette figure illustre les pratiques douteuses qui peuvent affecter nÃ©gativement l'intÃ©gritÃ© du cycle des dÃ©couvertes scientifiques. Figure adaptÃ©e d'un travail original par [scriberia](https://info.scriberia.com/contact-us) dans le cadre du livre [The Turing way](https://the-turing-way.netlify.app) sous licence [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/), DOI: [10.5281/zenodo.3332807](https://doi.org/10.5281/zenodo.3332807). La figure intÃ¨gre aussi une image [shutterstock](https://www.shutterstock.com/image-vector/computer-hacker-laptop-icon-787273936), utilisÃ©e sous licence shutterstock standard.
```
#### Biais de publication
La publication sÃ©lective est un de problÃ¨mes les plus importants identifiÃ©s dans le sondage vu plus tÃ´t. Cela signifie que les rÃ©sultats d'une Ã©tude ne sont publiÃ©s que lorsquâ€™ils ne sont positifs, c'est Ã  dire uniquement s'ils confirment les hypothÃ¨ses de l'Ã©quipe de recherche. Si ce type de pratique est systÃ©matique dans une communautÃ© de recherche, il se peut que plusieurs groupes rapporte un rÃ©sultat, qui semble alors  robuste, alors qu'en fait un nombre plus important de groupes de recherche n'ont pas pu rÃ©pliquer cet effet, mais sans publier. Cela vient dÃ©former complÃ¨tement les connaissances accumulÃ©es par la communautÃ© scientifique, qui est Ã  l'origine des hypothÃ¨ses des Ã©tudes futures.

##### p-hacking
Si on voit que nos rÃ©sultats ne correspondent pas Ã  nos attentes, on pourrait se demander si on n'a pas commis une erreur ou peut Ãªtre qu'on n'a pas choisi la technique d'analyse la plus optimale. On va alors revisiter la maniÃ¨re
dont analyse les donnÃ©es jusqu'Ã  ce que les rÃ©sultats deviennent significatifs. Ce type d'approche a Ã©tÃ© baptisÃ© _p-hacking_. Le p-hacking peut prendre de nombreuses formes: exclusion arbitraire de "valeurs aberrantes", sÃ©lection d'un sous-groupe qui montre l'effet attendu, changement des paramÃ¨tres de prÃ©traitements.

#### HARKing
La derniÃ¨re pratique douteuse est baptisÃ©e le Â« HARKing Â». Le terme HARK est un acronyme en anglais pour les termes Â« Hypothesis after results are known Â», ou bien "dÃ©finition des hypothÃ¨ses aprÃ¨s que les rÃ©sultats soient connus". On va effectuer de nombreux tests Ã  partir des donnÃ©es recueillies, et on formule a posteriori des hypothÃ¨ses correspondant aux rÃ©sultats significatifs dans l'Ã©chantillon. Ce processus n'est pas nÃ©cessairement malicieux, mais peut Ã©merger d'une volontÃ© d'interprÃ©ter les donnÃ©es. Cette dÃ©marche n'est pas nÃ©cessairement problÃ©matique, du moment que les hypothÃ¨ses sont (correctement) prÃ©sentÃ©es comme exploratoires, guidÃ©es par les donnÃ©es, plutÃ´t que comme une hypothÃ¨se a priori rigoureuse.

Nous allons maintenant voir comment la neuroimagerie reprÃ©sente un domaine particuliÃ¨rement propice au p-hacking, et d'autres facteurs qui contribuent au manque de reproductibilitÃ©. 

## ReproducibilitÃ© et neuroimagerie
On va maintenant voir quels problÃ¨mes sâ€™appliquent directement Ã  la
neuroimagerie et comment certains problÃ¨mes de reproductibilitÃ©
spÃ©cifiques peuvent ressortir lors de lâ€™utilisation de logiciels en
neuroimagerie.

### DegrÃ©s de libertÃ© en recherche
Le premier problÃ¨me qui est exacerbÃ© en neuroimagerie est le nombre
dâ€™Analyses quâ€™il faut faire ainsi que le nombre de choix quâ€™il faut faire en lien
avec ces analyses. Ceci est reliÃ© au problÃ¨me de p-hacking dont on a discutÃ©
plus tÃ´t. Ainsi, toutes les possibilitÃ©s dâ€™analyses permettent aux chercheurs
dâ€™aller faire ressortir lâ€™effet quâ€™ils dÃ©sirent observer dâ€™une faÃ§on ou dâ€™une
autre. Cette vaste sÃ©lection dâ€™analyses est le rÃ©sultat de dizaines de
paramÃ¨tres modifiables qui sont impliquÃ©s dans ces analyses. Une Ã©tude a
tentÃ© de systÃ©matiquement changer des choix de paramÃ¨tres dâ€™analyse pour
savoir quels rÃ©sultats ils pouvaient obtenir. En bout de ligne, ils ont obtenu
prÃ¨s de 7000 chaines de traitement qui pouvaient Ãªtre exÃ©cutÃ©es. Ensuite,
lâ€™Ã©quipe a essayÃ© diffÃ©rents seuils statistiques ce qui a gÃ©nÃ©rÃ© prÃ¨s de 35
000 cartes dâ€™activation. Ceci reprÃ©sente la variÃ©tÃ© des rÃ©sultats quâ€™il est
possible dâ€™obtenir en utilisant un simple contraste dâ€™images IRMf. On parle
aussi de Multiverse, dans le sens ou lâ€™histoire statistique peut changer
complÃ¨tement avec une seule petite modification dans le choix des
paramÃ¨tres dâ€™analyse. Dans le cas des images dâ€™IRM, on a tellement plus de
donnÃ©es et tellement plus dâ€™analyses a faire que le problÃ¨me de Multiverse
est dâ€™autant plus prononcÃ© comparativement aux analyses statistiques
traditionnelles.
On voit sur lâ€™image ici que les rÃ©gions qui prÃ©sentent la plus haute moyenne
sont aussi les rÃ©gions qui ont la plus grande variabilitÃ© dans les donnÃ©es,
mÃªme que lâ€™amplitude de la variation est plus grande que lâ€™amplitude de la
moyenne. Cela suggÃ¨re quâ€™il y a beaucoup de variabilitÃ© sur les cartes finales
par les diffÃ©rents choix analytiques choisis. Dans ce cas, les changements de
paramÃ¨tres ont Ã©tÃ© fait plutÃ´t systÃ©matiquement et les gens dans la vraie vie
ne jouent pas autant avec les paramÃ¨tres.

### Logiciels d'analyse
Maintenant, imaginons quâ€™on fait les analyses dans un des multiples logiciels
ci bas. Si les rÃ©sultats escomptÃ©s ne sont pas obtenus avec un des logiciels,
on pourrait aller faire les mÃªmes analyses avec un autre logiciel et obtenir
des rÃ©sultats diffÃ©rents.
Donc Ã  droite on a les logos des 3 logiciels principaux, AFNI, FSL et SPM. Par
ailleurs, il y a plein de versions de SPM. La premiÃ¨re version utilisÃ©e de
maniÃ¨re large est la version 95, aprÃ¨s il y a eu 96, 99, SPM8 et SMP 12. En
plus, chaque version de SPM a eu des versions mineures qui ne sont pas ici.
En fait, ce quâ€™il faut retenir, câ€™est que ce qui a Ã©tÃ© expliquÃ© en lien avec le
choix des analyses sâ€™applique aussi au choix du logiciel dâ€™analyse utilisÃ© en
IRM. MÃªme si on fait des choix comparables dâ€™analyses dans les diffÃ©rents
logiciels on va se retrouver avec des variations.
Donc une Ã©tude de Bowring a tentÃ© de prendre un jeu de donnÃ©es public et
des cartes dâ€™activation qui ont Ã©tÃ© gÃ©nÃ©rÃ©es dans un article et ils ont essayÃ©
de reproduire la carte dâ€™Activation de lâ€™article avec les 3 logiciels suivants :
FSL, AFNI, SPM. Les colonnes quâ€™on voit ici câ€™est 3 jeux de donnÃ©es diffÃ©rents,
en bas on a la carte dâ€™activation qui a publiÃ©e par le groupe dans lâ€™article
original et ce quâ€™on a dans chaque ligne au-dessus câ€™est ce qui a Ã©tÃ© obtenu
avec ces donnÃ©es Ã  lâ€™aide des 3 logiciels. Si on se concentre sur le premier
jeu de donnÃ©es Ã  gauche, on voit que finalement on voit des activations au
niveau du cingulaire antÃ©rieur et de lâ€™orbitofrontal ventral et une activation
nÃ©gative au niveau du cortex cingulaire postÃ©rieur. Avec FSL, on retrouve le
mÃªme genre dâ€™Activation mais avec des positions et des amplitudes
diffÃ©rentes. Des nouvelles activations ressortent au niveau occipital et au
niveau sous cortical qui ne sont pas lÃ  dans la carte dâ€™â€™activation publiÃ©e.
Pour SPM, lâ€™amplitude et la position des activations sont variables
comparativement a lâ€™original. Tant au niveau qualitatif que quantitatif, la
cohÃ©rence de ces 4 cartes dâ€™activation est assez faible. Câ€™est une belle
illustration, on voit quâ€™il y a un impact assez important du logiciel quâ€™on
utilise pour faire les cartes, ce qui en dit gros sur la reproductibilitÃ©. MÃªme
avec les mÃªmes donnÃ©es, seul le logiciel utilisÃ© pour les analyses peut avoir
un impact important sur les rÃ©sultats finaux des analyses. Quand on parlait
de p-hacking et du fait quâ€™on a plein dâ€™Ã©tapes qui peuvent Ãªtre modifiÃ©es, le
logiciel utilisÃ© sâ€™insÃ¨re dans ce problÃ¨me.

### Environnement de travail
De plus, les gens ont lâ€™habitude de tenir pour acquis que peu importe
lâ€™environnement ou on fait un calcul mathÃ©matique, on va obtenir le mÃªme
rÃ©sultat. En effet, si on calcul 4+6 dans Windows, sur mac os ou sur une
calculatrice, le rÃ©sultat donnera toujours 10. Or, les ordinateurs ne font pas
vraiment des calculs, ce qui change tout. Ils font des approximations avec un
nombre fini de chiffres aprÃ¨s la virgule. Parfois, quand les ordinateurs font
des calculs complexes, il y a plusieurs faÃ§ons de les implÃ©menter. En
pratique, quand on regarde nâ€™importe quelle mÃ©thode un peu complexe, il
est possible quâ€™on ait des diffÃ©rences entre les logiciels et les ordinateurs. Un
papier a regardÃ© lâ€™impact de la version de Mac OS sur les rÃ©sultats dâ€™une
analyse de morphomÃ©trie faite avec Freesurfer. Il y a diffÃ©rentes analyses
rapportÃ©es dans ce papier. Ce quâ€™on a sur ce graphique ce sont diffÃ©rentes
rÃ©gions gÃ©nÃ©rÃ©es par Freesurfer. Dans ces barres-lÃ , on a 3 versions de iOS X
(versions majeures et versions mineures). On voit la diffÃ©rence entre les
diffÃ©rentes versions de Mac en utilisant le mÃªme logiciel. On voit des
changements de 10% au niveau du cortex enthorinal seulement en fonction
du logiciel du systÃ¨me dâ€™exploitation ! Malheureusement, lâ€™environnement de
travail peut impacter la reproductibilitÃ© des rÃ©sultats. Ce nâ€™est pas propre Ã 
la neuroimagerie mais câ€™est particuliÃ¨rement problÃ©matique car on a de
grosses chaines de traitement et nos mÃ©thodes ne sont pas dÃ©veloppÃ©es par
des gens qui sâ€™y connaissent en traitement numÃ©rique, donc les gens ne sont
pas conscients des variations normales qui peuvent Ãªtre causÃ©es par
lâ€™utilisation dâ€™environnements variables.

### Tailles d'effet
Une autre erreur quâ€™on voit particuliÃ¨rement en neuroimagerie câ€™est
dâ€™interprÃ©ter une diffÃ©rence significative comme une diffÃ©rence importante.
Une diffÃ©rence significative signifierait que les populations Ã©tudiÃ©es sont
complÃ¨tement diffÃ©rentes. Par exemple, on trouve que les amygdales des
personnes sur le spectre de lâ€™autisme est plus petite que pour les gens
neurotypiques. Cela signifie que la diffÃ©rence de la moyenne des
distributions est diffÃ©rente, pas que tous les individus du groupe des gens
ayant un TSA ont une amygdale plus petite que les gens neurotypiques.
Malheureusement, beaucoup de chercheurs et de professionnels ont
tendance Ã  faire ce genre de conclusion. Une maniÃ¨re dâ€™aller voir la taille de
la diffÃ©rence est en regardant le d de Cohen. Un d de Cohen de 0.1 serait ce
quâ€™on va trouver si on remarque une diffÃ©rence significative entre nos
moyennes de groupe, tel quâ€™illustrer avec lâ€™exemple tes TSA plus tÃ´t. Un d de
Cohen de 0.79 dÃ©crirait un effet de groupe immense, ou le score de tous les
membres dâ€™un groupe est infÃ©rieur au score de tous les membres de lâ€™autre
groupe. Or, si on a un N trÃ¨s grand, mÃªme avec un d de Cohen de 0.1, on va
avoir un p de 0.000001. Il est important de comprendre que malgrÃ© la
significativitÃ© de la diffÃ©rence de groupe, le p ne nous dit rien quant Ã 
lâ€™importance de cette diffÃ©rence. Le vocabulaire est trÃ¨s important a
comprendre, car beaucoup de gens vont dire quâ€™il y a une forte diffÃ©rence en
raison du p, alors que cette valeur ne nous dit rien au sujet de la taille
dâ€™effet. En fait, ce quâ€™on doit retenir, câ€™est que si on score significatif nâ€™a pas
de taille dâ€™effet, elle nâ€™a pas de raison dâ€™Ãªtre rÃ©pliquÃ©. Pour rÃ©pliquer un effet,
il doit Ãªtre significatif et fort. En neuroimagerie surtout, on fait beaucoup de
tests statistiques alors on se retrouve souvent avec des p qui sont trÃ¨s
faibles. La majoritÃ© des papiers ne rapportent mÃªme pas les tailles dâ€™effet,
alors au final, personne ne regarde si lâ€™effet est fort. Or, cette question est
trÃ¨s importante car la taille de la diffÃ©rence a un impact sur la
reproductibilitÃ©

### MÃ©thodes incomplÃ¨tes
Une autre chose importante Ã  mentionner sont les mÃ©thodes incomplÃ¨tes.
Cette image a Ã©tÃ© faite par @redpenblackpen, un artiste de McGill. Il publie
des petits dessins sur le monde universitaire. Celui-ci est trÃ¨s intÃ©ressant. Le
rez de chaussÃ© reprÃ©sente le papier publiÃ©. On a une maison bien propre et
bien rangÃ©e. On va lire le papier et tout est carrÃ© et tout est rigoureux.
Parfois, on a du matÃ©riel supplÃ©mentaire qui est un document qui est moins
regardÃ© par les reviewers, contenant des vieilles affaires et du matÃ©riel qui a
Ã©tÃ© utilisÃ© pour le projet mais qui nâ€™est pas essentiel dans le papier principal.
Câ€™est un peu plus le bazar lÃ -dedans on voit parfois que beaucoup plus de
choses ont Ã©tÃ© analysÃ©es que ce qui a Ã©tÃ© prÃ©sentÃ© dans le papier. Ce qui est
intÃ©ressant câ€™est le 2e sous-sol, ou on voit le vrai travail avec les tentacules
multicolore et la pagaille. En rÃ©alitÃ©, des tonnes de choses ont Ã©tÃ© essayÃ©es,
certaines choses pas terminÃ©es et dâ€™autres oui mais oubliÃ©es. Au final,
lâ€™auteur lui-mÃªme serait incapable lui-mÃªme de se souvenir de tout ce qui a
Ã©tÃ© essayÃ© dans le processus du projet. De plus, la complexitÃ© des Ã©tapes
dâ€™analyse en neuroimageries rend encore plus difficile de se souvenir et de
comprendre ce qui a Ã©tÃ© fait. Parfois, le numÃ©ro du logiciel utilisÃ© nâ€™est
mÃªme pas rapportÃ© dans le papier et est enfoui loin dans la 2e cave. Ce que
cela veut dire, câ€™est que lâ€™article publiÃ© nous donne une idÃ©e trÃ¨s peu
dÃ©taillÃ©e et trÃ¨s peu complÃ¨te de ce qui a Ã©tÃ© fait. En fait, si les gens ne
partagent pas leur code et peur processus complet, il sera pratiquement
impossible de rÃ©pliquer les rÃ©sultats obtenus, justement en raison de la
profondeur de la 2e cave.

## Des solutions

### Ã‰tudes prÃ©-enregistrÃ©es
Solutions permettant de rÃ©pondre Ã  la crise de la reproductibilitÃ©
La premiÃ¨re idÃ©e, serait de modifier la maniÃ¨re dont on publie les articles. Un
des problÃ¨mes dans le cercle prÃ©sentÃ© au dÃ©part, câ€™est quâ€™il y a un temps
phÃ©nomÃ©nal entre lâ€™Ã©laboration des hypothÃ¨ses et la publication en plus de
ne pas avoir de publication des rÃ©sultats nÃ©gatifs. Une maniÃ¨re dâ€™Ã©liminer Ã§a,
câ€™est la publication des hypothÃ¨ses et des plans dâ€™analyse. Cela permet aux
reviewers de critiquer la conception de lâ€™Ã©tude avant quâ€™elle soit terminÃ©e,
donc permettrait les modifications Ã  priori si nÃ©cessaire. De plus, si les
rÃ©sultats ne collent pas aux hypothÃ¨ses, ce ne serait pas grave car lâ€™article
serait dÃ©jÃ  acceptÃ©. Il existe des articles prÃ©enregistrÃ©s, ou des registered
recall. Câ€™est comme un article normal, il manque seulement les rÃ©sultats et
la discussion. Une fois que les gens se sont mis dâ€™accord quant a la mÃ©thode,
lâ€™article est acceptÃ© peu importe les rÃ©sultats. Cela ne veut pas dire quâ€™on ne
peut pas prÃ©senter des nouvelles analyses auxquelles on nâ€™avait pas pensÃ©
avant. Celles-ci seront alors prÃ©sentÃ©es comme exploratoires comme il se
devrait, plutÃ´t que confirmatoires comme câ€™est souvent le cas dans les
articles dont la mÃ©thode nâ€™est pas acceptÃ©e et publiÃ©e dâ€™avance., ce quâ€™on
appelle le HARKing. Ce type dâ€™approche ne colle pas avec tout ce quâ€™on
souhaiterait publier. Ce ne fonctionne pas vraiment bien avec le travail
mÃ©thodologique, non plus avec les analyses plus exploratoires. Par contre,
pour les articles qui suivent la dÃ©marche traditionnelle, cela fonctionnerait
bien et rÃ©pondrait Ã  beaucoup des problÃ¨mes vus au dÃ©but de ce cours.

### Code
Une autre solution serait dâ€™apprendre Ã  coder. Automatiser les analyses
permet de les rendre plus facile pour quiconque de les reproduire. Il peut y
avoir des erreurs dans le code, mais elles peuvent Ãªtre vues et rÃ©parÃ©es
avec des traces. Les analyses qui ne reposent pas sur du code reprÃ©sente un
obstacle majeur Ã  la reproductibilitÃ©.

### Partage de code
Ensuite, partager ce code est un peu anxiogÃ¨ne. Souvent, les gens sont
rÃ©ticents a rendre public le code utilisÃ© pour gÃ©nÃ©rer un article. Comme câ€™est
une partie critique du travail de recherche, Ã§a vaut la peine dâ€™apprendre a le
faire comme il faut et de le partager pour aider Ã  rÃ©duire le problÃ¨me de
reproductibilitÃ©. Beaucoup de gens qui utilisent Github, une plateforme qui
permet de partager le code et aussi de partager les modifications qui y sont
faites avec le temps. En fait, la principale personne qui bÃ©nÃ©ficie de la
publicisation de son code est la personne qui le publie, car si le projet Ã©volue
dans le temps il y a traces de ce qui a Ã©tÃ© fait sur cette plateforme. Le
monstre du 2e sous-sol est alors un peu moins cachÃ© et un peu moins
inconnu pour vous et pour les personnes qui vous lisent Ã  la suite de la
publication. De plus en plus, on sâ€™attend que les scripts dâ€™analyse soient
rendus publics lors de la publication des papiers.

### Partage de donnÃ©es
Une autre solution est de partager les donnÃ©es. Cela facilite la vie des
laboratoires car 1 an et 2 ans aprÃ¨s avoir publiÃ© un papier, il est possible
quâ€™on ne se souvienne mÃªme plus ou se trouvent les donnÃ©es et quelle
version des donnÃ©es qui a Ã©tÃ© utilisÃ©e. Le partage de celles-ci rend plus facile
de se relire et de retrouver nos traces. Malheureusement, ce nâ€™est pas facile
de rendre nos donnÃ©es publiques. De plus en plus de gens poussent pour
rendre le partage de donnÃ©es plus commun et plus facile, mais ce nâ€™est pas
encore fait.
Partager ses donnÃ©es câ€™est comme un spectre qui est reprÃ©sentÃ© par ce
graphique. Sur lâ€™axe des y on a Ã  quel point câ€™est utilisable et sur lâ€™axe des x
câ€™est a quel point câ€™est beaucoup de donnÃ©es et Ã§a prend du temps a
prÃ©parer. En bas, on a les donnÃ©es ADNI ou HCP, des projets ou les gens
publient leurs donnÃ©es brutes et les donnÃ©es prÃ©traitÃ©es. Ensuite, on a
dâ€™autres personnes qui partagent uniquement leurs donnÃ©es brutes. Par la
suite, on a des gens qui partagent leurs cartes statistiques. Enfin, les
coordonnÃ©es des types dâ€™activation est quelque chose que beaucoup de
gens partagent dans les articles et qui est trÃ¨s utile, bien que beaucoup
moins riche que les cartes elles-mÃªmes. Le jour ou on arrivera a partager nos
donnÃ©es systÃ©matiquement avec nos articles on va avoir de grandes
amÃ©liorations au niveau de la reproductibilitÃ©.

### Partage d'environnement
Ensuite, on a des outils qui permettent de partager notre environnement de
travail. Une initiative qui est bien apprÃ©ciÃ©e est neurodebian, qui est une
version de Linux qui vient prÃ©installer avec un appstore pour la
neuroimagerie. On peut installer directement les logiciels et les systÃ¨mes
dâ€™opÃ©ration. Ainsi, quelquâ€™un qui veut reproduire votre environnement
pourrait le faire. Il y a aussi les containers qui permettent de garder tout
lâ€™environnement de travail sur un seul fichier, cela fonctionne sur Linux et il y
a des maniÃ¨res de le faire fonctionner sur Mac et sur Windows. Câ€™est
beaucoup utilisÃ© pour la programmation Web, et la communautÃ© scientifique
a commencÃ© Ã  lâ€™utiliser pour amÃ©liorer la reproductibilitÃ©.

### Bonnes pratiques
Certains articles se concentrent sur la formulation de Â« guides Â» de bonnes
pratiques pour diffÃ©rentes techniques et mÃ©thodes de recherche. Cela
permet de voir ce qui est le plus utile pour contrer la crise de reproductibilitÃ©
en fonction des mÃ©thodes les plus convoitÃ©es en neuroscience cognitive.
Une autre chose qui peut Ãªtre faite est dâ€™Ã©tudier la puissance statistique en
Ã©laborant un projet pour savoir si on doit modifier la taille de notre
Ã©chantillon. Cela peut Ãªtre fait avec diffÃ©rents logiciels et avec le site web
suivant : https://rpsychologist.com/d3/nhst/

### La mÃ©thode scientifique revisitÃ©e

### Vers une science gÃ©nÃ©ralisable

## Conclusions

## Exercices
